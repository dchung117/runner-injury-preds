{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd05637294743aae0e025520e0e9d1da7627837a77c00833e7443ec1a12437a0109",
   "display_name": "Python 3.9.5 64-bit ('runner-preds': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   nr. sessions  total km  km Z3-4  km Z5-T1-T2  km sprinting  strength training  hours alternative  perceived exertion  perceived trainingSuccess  perceived recovery  nr. sessions.1  total km.1  km Z3-4.1  km Z5-T1-T2.1  km sprinting.1  strength training.1  hours alternative.1  perceived exertion.1  perceived trainingSuccess.1  perceived recovery.1  nr. sessions.2  total km.2  km Z3-4.2  km Z5-T1-T2.2  km sprinting.2  strength training.2  hours alternative.2  perceived exertion.2  perceived trainingSuccess.2  perceived recovery.2  nr. sessions.3  total km.3  km Z3-4.3  km Z5-T1-T2.3  km sprinting.3  strength training.3  hours alternative.3  perceived exertion.3  perceived trainingSuccess.3  perceived recovery.3  nr. sessions.4  total km.4  km Z3-4.4  km Z5-T1-T2.4  km sprinting.4  strength training.4  hours alternative.4  perceived exertion.4  perceived trainingSuccess.4  perceived recovery.4  nr. sessions.5  total km.5  km Z3-4.5  km Z5-T1-T2.5  km sprinting.5  strength training.5  \\\n",
       "0           1.0       5.8      0.0          0.6           1.2                0.0               0.00                0.11                       0.00                0.18             0.0         0.0        0.0            0.0             0.0                  0.0                 0.00                 -0.01                        -0.01                 -0.01             1.0         0.0        0.0            0.0             0.0                  1.0                 0.00                  0.10                         0.00                  0.17             0.0         0.0        0.0            0.0             0.0                  0.0                 0.00                 -0.01                        -0.01                 -0.01             1.0         0.0        0.0            0.0             0.0                  0.0                 1.08                  0.08                         0.00                  0.18             1.0        16.4       10.0            0.0             0.0                  1.0   \n",
       "1           0.0       0.0      0.0          0.0           0.0                0.0               0.00               -0.01                      -0.01               -0.01             1.0         0.0        0.0            0.0             0.0                  1.0                 0.00                  0.10                         0.00                  0.17             0.0         0.0        0.0            0.0             0.0                  0.0                 0.00                 -0.01                        -0.01                 -0.01             1.0         0.0        0.0            0.0             0.0                  0.0                 1.08                  0.08                         0.00                  0.18             1.0        16.4       10.0            0.0             0.0                  1.0                 0.00                  0.11                         0.00                  0.17             1.0         0.0        0.0            0.0             0.0                  0.0   \n",
       "2           1.0       0.0      0.0          0.0           0.0                1.0               0.00                0.10                       0.00                0.17             0.0         0.0        0.0            0.0             0.0                  0.0                 0.00                 -0.01                        -0.01                 -0.01             1.0         0.0        0.0            0.0             0.0                  0.0                 1.08                  0.08                         0.00                  0.18             1.0        16.4       10.0            0.0             0.0                  1.0                 0.00                  0.11                         0.00                  0.17             1.0         0.0        0.0            0.0             0.0                  0.0                 1.00                  0.10                         0.00                  0.15             1.0         5.2        0.0            0.5             1.2                  0.0   \n",
       "3           0.0       0.0      0.0          0.0           0.0                0.0               0.00               -0.01                      -0.01               -0.01             1.0         0.0        0.0            0.0             0.0                  0.0                 1.08                  0.08                         0.00                  0.18             1.0        16.4       10.0            0.0             0.0                  1.0                 0.00                  0.11                         0.00                  0.17             1.0         0.0        0.0            0.0             0.0                  0.0                 1.00                  0.10                         0.00                  0.15             1.0         5.2        0.0            0.5             1.2                  0.0                 0.00                  0.10                         0.00                  0.17             0.0         0.0        0.0            0.0             0.0                  0.0   \n",
       "4           1.0       0.0      0.0          0.0           0.0                0.0               1.08                0.08                       0.00                0.18             1.0        16.4       10.0            0.0             0.0                  1.0                 0.00                  0.11                         0.00                  0.17             1.0         0.0        0.0            0.0             0.0                  0.0                 1.00                  0.10                         0.00                  0.15             1.0         5.2        0.0            0.5             1.2                  0.0                 0.00                  0.10                         0.00                  0.17             0.0         0.0        0.0            0.0             0.0                  0.0                 0.00                 -0.01                        -0.01                 -0.01             1.0         0.0        0.0            0.0             0.0                  1.0   \n",
       "\n",
       "   hours alternative.5  perceived exertion.5  perceived trainingSuccess.5  perceived recovery.5  nr. sessions.6  total km.6  km Z3-4.6  km Z5-T1-T2.6  km sprinting.6  strength training.6  hours alternative.6  perceived exertion.6  perceived trainingSuccess.6  perceived recovery.6  Athlete ID  injury  Date  \n",
       "0                  0.0                  0.11                         0.00                  0.17             1.0         0.0        0.0            0.0             0.0                  0.0                  1.0                  0.10                         0.00                  0.15           0       0     0  \n",
       "1                  1.0                  0.10                         0.00                  0.15             1.0         5.2        0.0            0.5             1.2                  0.0                  0.0                  0.10                         0.00                  0.17           0       0     1  \n",
       "2                  0.0                  0.10                         0.00                  0.17             0.0         0.0        0.0            0.0             0.0                  0.0                  0.0                 -0.01                        -0.01                 -0.01           0       0     2  \n",
       "3                  0.0                 -0.01                        -0.01                 -0.01             1.0         0.0        0.0            0.0             0.0                  1.0                  0.0                  0.10                         0.00                  0.17           0       0     3  \n",
       "4                  0.0                  0.10                         0.00                  0.17             1.0        17.6        7.2            0.0             0.0                  0.0                  0.0                  0.11                         0.00                  0.17           0       0     4  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nr. sessions</th>\n      <th>total km</th>\n      <th>km Z3-4</th>\n      <th>km Z5-T1-T2</th>\n      <th>km sprinting</th>\n      <th>strength training</th>\n      <th>hours alternative</th>\n      <th>perceived exertion</th>\n      <th>perceived trainingSuccess</th>\n      <th>perceived recovery</th>\n      <th>nr. sessions.1</th>\n      <th>total km.1</th>\n      <th>km Z3-4.1</th>\n      <th>km Z5-T1-T2.1</th>\n      <th>km sprinting.1</th>\n      <th>strength training.1</th>\n      <th>hours alternative.1</th>\n      <th>perceived exertion.1</th>\n      <th>perceived trainingSuccess.1</th>\n      <th>perceived recovery.1</th>\n      <th>nr. sessions.2</th>\n      <th>total km.2</th>\n      <th>km Z3-4.2</th>\n      <th>km Z5-T1-T2.2</th>\n      <th>km sprinting.2</th>\n      <th>strength training.2</th>\n      <th>hours alternative.2</th>\n      <th>perceived exertion.2</th>\n      <th>perceived trainingSuccess.2</th>\n      <th>perceived recovery.2</th>\n      <th>nr. sessions.3</th>\n      <th>total km.3</th>\n      <th>km Z3-4.3</th>\n      <th>km Z5-T1-T2.3</th>\n      <th>km sprinting.3</th>\n      <th>strength training.3</th>\n      <th>hours alternative.3</th>\n      <th>perceived exertion.3</th>\n      <th>perceived trainingSuccess.3</th>\n      <th>perceived recovery.3</th>\n      <th>nr. sessions.4</th>\n      <th>total km.4</th>\n      <th>km Z3-4.4</th>\n      <th>km Z5-T1-T2.4</th>\n      <th>km sprinting.4</th>\n      <th>strength training.4</th>\n      <th>hours alternative.4</th>\n      <th>perceived exertion.4</th>\n      <th>perceived trainingSuccess.4</th>\n      <th>perceived recovery.4</th>\n      <th>nr. sessions.5</th>\n      <th>total km.5</th>\n      <th>km Z3-4.5</th>\n      <th>km Z5-T1-T2.5</th>\n      <th>km sprinting.5</th>\n      <th>strength training.5</th>\n      <th>hours alternative.5</th>\n      <th>perceived exertion.5</th>\n      <th>perceived trainingSuccess.5</th>\n      <th>perceived recovery.5</th>\n      <th>nr. sessions.6</th>\n      <th>total km.6</th>\n      <th>km Z3-4.6</th>\n      <th>km Z5-T1-T2.6</th>\n      <th>km sprinting.6</th>\n      <th>strength training.6</th>\n      <th>hours alternative.6</th>\n      <th>perceived exertion.6</th>\n      <th>perceived trainingSuccess.6</th>\n      <th>perceived recovery.6</th>\n      <th>Athlete ID</th>\n      <th>injury</th>\n      <th>Date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>5.8</td>\n      <td>0.0</td>\n      <td>0.6</td>\n      <td>1.2</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>0.18</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.08</td>\n      <td>0.08</td>\n      <td>0.00</td>\n      <td>0.18</td>\n      <td>1.0</td>\n      <td>16.4</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.08</td>\n      <td>0.08</td>\n      <td>0.00</td>\n      <td>0.18</td>\n      <td>1.0</td>\n      <td>16.4</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.15</td>\n      <td>1.0</td>\n      <td>5.2</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>1.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.08</td>\n      <td>0.08</td>\n      <td>0.00</td>\n      <td>0.18</td>\n      <td>1.0</td>\n      <td>16.4</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.15</td>\n      <td>1.0</td>\n      <td>5.2</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>1.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.08</td>\n      <td>0.08</td>\n      <td>0.00</td>\n      <td>0.18</td>\n      <td>1.0</td>\n      <td>16.4</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.15</td>\n      <td>1.0</td>\n      <td>5.2</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>1.2</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.08</td>\n      <td>0.08</td>\n      <td>0.00</td>\n      <td>0.18</td>\n      <td>1.0</td>\n      <td>16.4</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.15</td>\n      <td>1.0</td>\n      <td>5.2</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>1.2</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>1.0</td>\n      <td>17.6</td>\n      <td>7.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Read in daily data\n",
    "data_path = os.path.join('.', 'data')\n",
    "daily_data_path = os.path.join(data_path, 'day_approach_maskedID_timeseries.csv')\n",
    "weekly_data_path = os.path.join(data_path, 'week_approach_maskedID_timeseries.csv')\n",
    "\n",
    "daily_df = pd.read_csv(daily_data_path)\n",
    "daily_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "target_col = 'injury'\n",
    "feat_cols = [c for c in daily_df.columns if c not in ['Athlete ID', 'injury', 'Date']]\n",
    "\n",
    "X = daily_df[feat_cols].values\n",
    "y = daily_df[target_col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions for cross validation - ensure train/test splits preserve class imbalance of full dataset\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "def run_cv(model, X, y, scoring, n_splits, verbose=0, return_estimators=False):\n",
    "    cv_results = cross_validate(model, X, y, scoring=scoring, cv=StratifiedKFold(n_splits=n_splits), verbose=verbose, return_estimator=return_estimators)\n",
    "    return cv_results\n",
    "\n",
    "def display_results(results_dict):\n",
    "    for k in results_dict:\n",
    "        if isinstance(results_dict[k], type(np.array([]))):\n",
    "            print(f\"Average {k.capitalize()}: {results_dict[k].mean():04f}\")\n",
    "            print()"
   ]
  },
  {
   "source": [
    "## Linear regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average Fit_time: 0.114480\n\nAverage Score_time: 0.002161\n\nAverage Test_score: -0.116197\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "cv_scores = run_cv(lin_reg, X, y, scoring='neg_root_mean_squared_error', n_splits=5, verbose=0, return_estimators=True)\n",
    "display_results(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       nr. sessions  total km   km Z3-4  km Z5-T1-T2  km sprinting  strength training  hours alternative  perceived exertion  perceived trainingSuccess  perceived recovery  nr. sessions.1  total km.1  km Z3-4.1  km Z5-T1-T2.1  km sprinting.1  strength training.1  hours alternative.1  perceived exertion.1  perceived trainingSuccess.1  perceived recovery.1  nr. sessions.2  total km.2  km Z3-4.2  km Z5-T1-T2.2  km sprinting.2  strength training.2  hours alternative.2  perceived exertion.2  perceived trainingSuccess.2  perceived recovery.2  nr. sessions.3  total km.3  km Z3-4.3  km Z5-T1-T2.3  km sprinting.3  strength training.3  hours alternative.3  perceived exertion.3  perceived trainingSuccess.3  perceived recovery.3  nr. sessions.4  total km.4  km Z3-4.4  km Z5-T1-T2.4  km sprinting.4  strength training.4  hours alternative.4  perceived exertion.4  perceived trainingSuccess.4  perceived recovery.4  nr. sessions.5    total km.5  km Z3-4.5  km Z5-T1-T2.5  km sprinting.5  \\\n",
       "count      5.000000  5.000000  5.000000     5.000000      5.000000           5.000000           5.000000            5.000000                   5.000000            5.000000        5.000000    5.000000   5.000000       5.000000        5.000000             5.000000             5.000000              5.000000                     5.000000              5.000000        5.000000    5.000000   5.000000       5.000000        5.000000             5.000000             5.000000              5.000000                     5.000000              5.000000        5.000000    5.000000   5.000000       5.000000        5.000000             5.000000             5.000000              5.000000                     5.000000              5.000000        5.000000    5.000000   5.000000       5.000000        5.000000             5.000000             5.000000              5.000000                     5.000000              5.000000        5.000000  5.000000e+00   5.000000       5.000000        5.000000   \n",
       "mean       0.002764 -0.000033 -0.000222     0.000437      0.001730          -0.000733          -0.003400            0.003084                   0.000587            0.000606       -0.001330   -0.000264   0.000256       0.000214        0.001498             0.006625             0.001319             -0.000495                     0.008083             -0.009135        0.002074   -0.000310   0.000910       0.000286        0.002559             0.000934            -0.001048              0.003088                    -0.002426              0.001680       -0.001216   -0.000148   0.000250       0.000432        0.001676             0.001128            -0.000878              0.006421                     0.005077             -0.003820        0.003240   -0.000229   0.000498       0.000740       -0.000868            -0.002946            -0.002399              0.003571                     0.002928              0.000199       -0.000974  5.880961e-05   0.000891       0.001133        0.000882   \n",
       "std        0.001498  0.000148  0.000157     0.000296      0.001621           0.001753           0.000708            0.002368                   0.000567            0.005019        0.001184    0.000087   0.000141       0.000195        0.002316             0.002164             0.000717              0.002859                     0.001933              0.002509        0.001307    0.000059   0.000071       0.000251        0.002409             0.002113             0.000767              0.002253                     0.001819              0.003970        0.000602    0.000101   0.000140       0.000175        0.002541             0.000398             0.000480              0.004412                     0.001597              0.004227        0.000699    0.000049   0.000128       0.000146        0.000733             0.001121             0.000590              0.001303                     0.001396              0.004502        0.000793  1.351316e-04   0.000227       0.000170        0.000389   \n",
       "min        0.000732 -0.000179 -0.000455    -0.000084      0.000112          -0.002698          -0.004025            0.001095                  -0.000376           -0.005039       -0.002682   -0.000359   0.000018      -0.000006       -0.000380             0.003297             0.000190             -0.004710                     0.005496             -0.013030        0.000796   -0.000384   0.000832      -0.000077        0.000256            -0.001401            -0.002291              0.001270                    -0.004241             -0.004136       -0.002108   -0.000253   0.000117       0.000132       -0.000837             0.000763            -0.001309              0.002648                     0.003014             -0.008071        0.002829   -0.000272   0.000388       0.000597       -0.001551            -0.004029            -0.003218              0.002342                     0.000435             -0.005205       -0.001830 -4.041327e-05   0.000676       0.000953        0.000294   \n",
       "25%        0.001709 -0.000169 -0.000314     0.000468      0.000860          -0.001248          -0.003989            0.002168                   0.000547           -0.001464       -0.002325   -0.000335   0.000237       0.000027        0.000153             0.005987             0.001237             -0.001274                     0.007388             -0.009197        0.001269   -0.000358   0.000845       0.000221        0.001735            -0.000373            -0.001114              0.001385                    -0.002916              0.000489       -0.001506   -0.000210   0.000134       0.000444        0.000904             0.000765            -0.001253              0.003990                     0.004208             -0.006710        0.002838   -0.000255   0.000392       0.000627       -0.001371            -0.004007            -0.002735              0.002661                     0.003444             -0.001950       -0.001374 -5.025660e-06   0.000779       0.001071        0.000729   \n",
       "50%        0.003330 -0.000025 -0.000131     0.000598      0.000995          -0.000925          -0.003407            0.002210                   0.000837           -0.001294       -0.001331   -0.000276   0.000327       0.000270        0.000830             0.006793             0.001287             -0.000278                     0.007555             -0.009112        0.001961   -0.000295   0.000917       0.000266        0.001953             0.000430            -0.000993              0.001800                    -0.002828              0.002406       -0.001069   -0.000180   0.000235       0.000477        0.001150             0.001189            -0.000950              0.004450                     0.005007             -0.005352        0.002913   -0.000252   0.000459       0.000701       -0.000912            -0.003045            -0.002235              0.002893                     0.003484             -0.000423       -0.001191 -9.726858e-07   0.000829       0.001101        0.000949   \n",
       "75%        0.003678  0.000033 -0.000118     0.000601      0.002501          -0.000906          -0.003302            0.002767                   0.000897            0.002696       -0.000294   -0.000187   0.000332       0.000343        0.001432             0.008501             0.001922              0.000701                     0.009646             -0.008233        0.002137   -0.000261   0.000952       0.000422        0.002201             0.002046            -0.000497              0.004838                    -0.002786              0.002878       -0.000768   -0.000100   0.000304       0.000531        0.001196             0.001196            -0.000755              0.007316                     0.005992             -0.001077        0.003149   -0.000217   0.000565       0.000831       -0.000833            -0.002148            -0.002115              0.004859                     0.003605              0.001689       -0.000732  4.629588e-05   0.000902       0.001128        0.001179   \n",
       "max        0.004371  0.000176 -0.000093     0.000601      0.004180           0.002112          -0.002280            0.007179                   0.001032            0.008132       -0.000018   -0.000162   0.000367       0.000436        0.005456             0.008548             0.001960              0.003087                     0.010331             -0.006104        0.004205   -0.000252   0.001001       0.000599        0.006649             0.003969            -0.000344              0.006149                     0.000641              0.006763       -0.000627    0.000001   0.000459       0.000578        0.005968             0.001727            -0.000120              0.013703                     0.007163              0.002111        0.004469   -0.000148   0.000689       0.000944        0.000325            -0.001502            -0.001692              0.005098                     0.003670              0.006881        0.000258  2.941638e-04   0.001270       0.001414        0.001259   \n",
       "\n",
       "       strength training.5  hours alternative.5  perceived exertion.5  perceived trainingSuccess.5  perceived recovery.5  nr. sessions.6  total km.6  km Z3-4.6  km Z5-T1-T2.6  km sprinting.6  strength training.6  hours alternative.6  perceived exertion.6  perceived trainingSuccess.6  perceived recovery.6  \n",
       "count             5.000000             5.000000              5.000000                     5.000000              5.000000        5.000000    5.000000   5.000000       5.000000        5.000000             5.000000             5.000000              5.000000                     5.000000              5.000000  \n",
       "mean             -0.001077            -0.001818              0.005620                    -0.005537              0.006350        0.002400   -0.000159   0.000519       0.000878        0.003687             0.000205             0.002975              0.004123                    -0.007136              0.004208  \n",
       "std               0.001269             0.000821              0.003115                     0.002684              0.003749        0.001532    0.000110   0.000102       0.000215        0.002575             0.002007             0.001999              0.001789                     0.001055              0.000555  \n",
       "min              -0.003268            -0.002980              0.002468                    -0.008502              0.003291        0.000075   -0.000326   0.000395       0.000527        0.002060            -0.003161             0.000083              0.001863                    -0.008480              0.003758  \n",
       "25%              -0.001087            -0.002015              0.004078                    -0.007702              0.003922        0.002256   -0.000187   0.000451       0.000858        0.002519             0.000108             0.002415              0.003516                    -0.008058              0.003847  \n",
       "50%              -0.000448            -0.001937              0.004193                    -0.005596              0.005729        0.002590   -0.000148   0.000524       0.000913        0.002615             0.000926             0.002903              0.003628                    -0.006569              0.004073  \n",
       "75%              -0.000376            -0.001399              0.006992                    -0.003909              0.006095        0.002725   -0.000103   0.000571       0.001010        0.002984             0.001086             0.003995              0.004958                    -0.006412              0.004215  \n",
       "max              -0.000208            -0.000760              0.010372                    -0.001975              0.012714        0.004354   -0.000032   0.000656       0.001082        0.008255             0.002068             0.005477              0.006650                    -0.006163              0.005146  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nr. sessions</th>\n      <th>total km</th>\n      <th>km Z3-4</th>\n      <th>km Z5-T1-T2</th>\n      <th>km sprinting</th>\n      <th>strength training</th>\n      <th>hours alternative</th>\n      <th>perceived exertion</th>\n      <th>perceived trainingSuccess</th>\n      <th>perceived recovery</th>\n      <th>nr. sessions.1</th>\n      <th>total km.1</th>\n      <th>km Z3-4.1</th>\n      <th>km Z5-T1-T2.1</th>\n      <th>km sprinting.1</th>\n      <th>strength training.1</th>\n      <th>hours alternative.1</th>\n      <th>perceived exertion.1</th>\n      <th>perceived trainingSuccess.1</th>\n      <th>perceived recovery.1</th>\n      <th>nr. sessions.2</th>\n      <th>total km.2</th>\n      <th>km Z3-4.2</th>\n      <th>km Z5-T1-T2.2</th>\n      <th>km sprinting.2</th>\n      <th>strength training.2</th>\n      <th>hours alternative.2</th>\n      <th>perceived exertion.2</th>\n      <th>perceived trainingSuccess.2</th>\n      <th>perceived recovery.2</th>\n      <th>nr. sessions.3</th>\n      <th>total km.3</th>\n      <th>km Z3-4.3</th>\n      <th>km Z5-T1-T2.3</th>\n      <th>km sprinting.3</th>\n      <th>strength training.3</th>\n      <th>hours alternative.3</th>\n      <th>perceived exertion.3</th>\n      <th>perceived trainingSuccess.3</th>\n      <th>perceived recovery.3</th>\n      <th>nr. sessions.4</th>\n      <th>total km.4</th>\n      <th>km Z3-4.4</th>\n      <th>km Z5-T1-T2.4</th>\n      <th>km sprinting.4</th>\n      <th>strength training.4</th>\n      <th>hours alternative.4</th>\n      <th>perceived exertion.4</th>\n      <th>perceived trainingSuccess.4</th>\n      <th>perceived recovery.4</th>\n      <th>nr. sessions.5</th>\n      <th>total km.5</th>\n      <th>km Z3-4.5</th>\n      <th>km Z5-T1-T2.5</th>\n      <th>km sprinting.5</th>\n      <th>strength training.5</th>\n      <th>hours alternative.5</th>\n      <th>perceived exertion.5</th>\n      <th>perceived trainingSuccess.5</th>\n      <th>perceived recovery.5</th>\n      <th>nr. sessions.6</th>\n      <th>total km.6</th>\n      <th>km Z3-4.6</th>\n      <th>km Z5-T1-T2.6</th>\n      <th>km sprinting.6</th>\n      <th>strength training.6</th>\n      <th>hours alternative.6</th>\n      <th>perceived exertion.6</th>\n      <th>perceived trainingSuccess.6</th>\n      <th>perceived recovery.6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000e+00</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.002764</td>\n      <td>-0.000033</td>\n      <td>-0.000222</td>\n      <td>0.000437</td>\n      <td>0.001730</td>\n      <td>-0.000733</td>\n      <td>-0.003400</td>\n      <td>0.003084</td>\n      <td>0.000587</td>\n      <td>0.000606</td>\n      <td>-0.001330</td>\n      <td>-0.000264</td>\n      <td>0.000256</td>\n      <td>0.000214</td>\n      <td>0.001498</td>\n      <td>0.006625</td>\n      <td>0.001319</td>\n      <td>-0.000495</td>\n      <td>0.008083</td>\n      <td>-0.009135</td>\n      <td>0.002074</td>\n      <td>-0.000310</td>\n      <td>0.000910</td>\n      <td>0.000286</td>\n      <td>0.002559</td>\n      <td>0.000934</td>\n      <td>-0.001048</td>\n      <td>0.003088</td>\n      <td>-0.002426</td>\n      <td>0.001680</td>\n      <td>-0.001216</td>\n      <td>-0.000148</td>\n      <td>0.000250</td>\n      <td>0.000432</td>\n      <td>0.001676</td>\n      <td>0.001128</td>\n      <td>-0.000878</td>\n      <td>0.006421</td>\n      <td>0.005077</td>\n      <td>-0.003820</td>\n      <td>0.003240</td>\n      <td>-0.000229</td>\n      <td>0.000498</td>\n      <td>0.000740</td>\n      <td>-0.000868</td>\n      <td>-0.002946</td>\n      <td>-0.002399</td>\n      <td>0.003571</td>\n      <td>0.002928</td>\n      <td>0.000199</td>\n      <td>-0.000974</td>\n      <td>5.880961e-05</td>\n      <td>0.000891</td>\n      <td>0.001133</td>\n      <td>0.000882</td>\n      <td>-0.001077</td>\n      <td>-0.001818</td>\n      <td>0.005620</td>\n      <td>-0.005537</td>\n      <td>0.006350</td>\n      <td>0.002400</td>\n      <td>-0.000159</td>\n      <td>0.000519</td>\n      <td>0.000878</td>\n      <td>0.003687</td>\n      <td>0.000205</td>\n      <td>0.002975</td>\n      <td>0.004123</td>\n      <td>-0.007136</td>\n      <td>0.004208</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.001498</td>\n      <td>0.000148</td>\n      <td>0.000157</td>\n      <td>0.000296</td>\n      <td>0.001621</td>\n      <td>0.001753</td>\n      <td>0.000708</td>\n      <td>0.002368</td>\n      <td>0.000567</td>\n      <td>0.005019</td>\n      <td>0.001184</td>\n      <td>0.000087</td>\n      <td>0.000141</td>\n      <td>0.000195</td>\n      <td>0.002316</td>\n      <td>0.002164</td>\n      <td>0.000717</td>\n      <td>0.002859</td>\n      <td>0.001933</td>\n      <td>0.002509</td>\n      <td>0.001307</td>\n      <td>0.000059</td>\n      <td>0.000071</td>\n      <td>0.000251</td>\n      <td>0.002409</td>\n      <td>0.002113</td>\n      <td>0.000767</td>\n      <td>0.002253</td>\n      <td>0.001819</td>\n      <td>0.003970</td>\n      <td>0.000602</td>\n      <td>0.000101</td>\n      <td>0.000140</td>\n      <td>0.000175</td>\n      <td>0.002541</td>\n      <td>0.000398</td>\n      <td>0.000480</td>\n      <td>0.004412</td>\n      <td>0.001597</td>\n      <td>0.004227</td>\n      <td>0.000699</td>\n      <td>0.000049</td>\n      <td>0.000128</td>\n      <td>0.000146</td>\n      <td>0.000733</td>\n      <td>0.001121</td>\n      <td>0.000590</td>\n      <td>0.001303</td>\n      <td>0.001396</td>\n      <td>0.004502</td>\n      <td>0.000793</td>\n      <td>1.351316e-04</td>\n      <td>0.000227</td>\n      <td>0.000170</td>\n      <td>0.000389</td>\n      <td>0.001269</td>\n      <td>0.000821</td>\n      <td>0.003115</td>\n      <td>0.002684</td>\n      <td>0.003749</td>\n      <td>0.001532</td>\n      <td>0.000110</td>\n      <td>0.000102</td>\n      <td>0.000215</td>\n      <td>0.002575</td>\n      <td>0.002007</td>\n      <td>0.001999</td>\n      <td>0.001789</td>\n      <td>0.001055</td>\n      <td>0.000555</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000732</td>\n      <td>-0.000179</td>\n      <td>-0.000455</td>\n      <td>-0.000084</td>\n      <td>0.000112</td>\n      <td>-0.002698</td>\n      <td>-0.004025</td>\n      <td>0.001095</td>\n      <td>-0.000376</td>\n      <td>-0.005039</td>\n      <td>-0.002682</td>\n      <td>-0.000359</td>\n      <td>0.000018</td>\n      <td>-0.000006</td>\n      <td>-0.000380</td>\n      <td>0.003297</td>\n      <td>0.000190</td>\n      <td>-0.004710</td>\n      <td>0.005496</td>\n      <td>-0.013030</td>\n      <td>0.000796</td>\n      <td>-0.000384</td>\n      <td>0.000832</td>\n      <td>-0.000077</td>\n      <td>0.000256</td>\n      <td>-0.001401</td>\n      <td>-0.002291</td>\n      <td>0.001270</td>\n      <td>-0.004241</td>\n      <td>-0.004136</td>\n      <td>-0.002108</td>\n      <td>-0.000253</td>\n      <td>0.000117</td>\n      <td>0.000132</td>\n      <td>-0.000837</td>\n      <td>0.000763</td>\n      <td>-0.001309</td>\n      <td>0.002648</td>\n      <td>0.003014</td>\n      <td>-0.008071</td>\n      <td>0.002829</td>\n      <td>-0.000272</td>\n      <td>0.000388</td>\n      <td>0.000597</td>\n      <td>-0.001551</td>\n      <td>-0.004029</td>\n      <td>-0.003218</td>\n      <td>0.002342</td>\n      <td>0.000435</td>\n      <td>-0.005205</td>\n      <td>-0.001830</td>\n      <td>-4.041327e-05</td>\n      <td>0.000676</td>\n      <td>0.000953</td>\n      <td>0.000294</td>\n      <td>-0.003268</td>\n      <td>-0.002980</td>\n      <td>0.002468</td>\n      <td>-0.008502</td>\n      <td>0.003291</td>\n      <td>0.000075</td>\n      <td>-0.000326</td>\n      <td>0.000395</td>\n      <td>0.000527</td>\n      <td>0.002060</td>\n      <td>-0.003161</td>\n      <td>0.000083</td>\n      <td>0.001863</td>\n      <td>-0.008480</td>\n      <td>0.003758</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.001709</td>\n      <td>-0.000169</td>\n      <td>-0.000314</td>\n      <td>0.000468</td>\n      <td>0.000860</td>\n      <td>-0.001248</td>\n      <td>-0.003989</td>\n      <td>0.002168</td>\n      <td>0.000547</td>\n      <td>-0.001464</td>\n      <td>-0.002325</td>\n      <td>-0.000335</td>\n      <td>0.000237</td>\n      <td>0.000027</td>\n      <td>0.000153</td>\n      <td>0.005987</td>\n      <td>0.001237</td>\n      <td>-0.001274</td>\n      <td>0.007388</td>\n      <td>-0.009197</td>\n      <td>0.001269</td>\n      <td>-0.000358</td>\n      <td>0.000845</td>\n      <td>0.000221</td>\n      <td>0.001735</td>\n      <td>-0.000373</td>\n      <td>-0.001114</td>\n      <td>0.001385</td>\n      <td>-0.002916</td>\n      <td>0.000489</td>\n      <td>-0.001506</td>\n      <td>-0.000210</td>\n      <td>0.000134</td>\n      <td>0.000444</td>\n      <td>0.000904</td>\n      <td>0.000765</td>\n      <td>-0.001253</td>\n      <td>0.003990</td>\n      <td>0.004208</td>\n      <td>-0.006710</td>\n      <td>0.002838</td>\n      <td>-0.000255</td>\n      <td>0.000392</td>\n      <td>0.000627</td>\n      <td>-0.001371</td>\n      <td>-0.004007</td>\n      <td>-0.002735</td>\n      <td>0.002661</td>\n      <td>0.003444</td>\n      <td>-0.001950</td>\n      <td>-0.001374</td>\n      <td>-5.025660e-06</td>\n      <td>0.000779</td>\n      <td>0.001071</td>\n      <td>0.000729</td>\n      <td>-0.001087</td>\n      <td>-0.002015</td>\n      <td>0.004078</td>\n      <td>-0.007702</td>\n      <td>0.003922</td>\n      <td>0.002256</td>\n      <td>-0.000187</td>\n      <td>0.000451</td>\n      <td>0.000858</td>\n      <td>0.002519</td>\n      <td>0.000108</td>\n      <td>0.002415</td>\n      <td>0.003516</td>\n      <td>-0.008058</td>\n      <td>0.003847</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.003330</td>\n      <td>-0.000025</td>\n      <td>-0.000131</td>\n      <td>0.000598</td>\n      <td>0.000995</td>\n      <td>-0.000925</td>\n      <td>-0.003407</td>\n      <td>0.002210</td>\n      <td>0.000837</td>\n      <td>-0.001294</td>\n      <td>-0.001331</td>\n      <td>-0.000276</td>\n      <td>0.000327</td>\n      <td>0.000270</td>\n      <td>0.000830</td>\n      <td>0.006793</td>\n      <td>0.001287</td>\n      <td>-0.000278</td>\n      <td>0.007555</td>\n      <td>-0.009112</td>\n      <td>0.001961</td>\n      <td>-0.000295</td>\n      <td>0.000917</td>\n      <td>0.000266</td>\n      <td>0.001953</td>\n      <td>0.000430</td>\n      <td>-0.000993</td>\n      <td>0.001800</td>\n      <td>-0.002828</td>\n      <td>0.002406</td>\n      <td>-0.001069</td>\n      <td>-0.000180</td>\n      <td>0.000235</td>\n      <td>0.000477</td>\n      <td>0.001150</td>\n      <td>0.001189</td>\n      <td>-0.000950</td>\n      <td>0.004450</td>\n      <td>0.005007</td>\n      <td>-0.005352</td>\n      <td>0.002913</td>\n      <td>-0.000252</td>\n      <td>0.000459</td>\n      <td>0.000701</td>\n      <td>-0.000912</td>\n      <td>-0.003045</td>\n      <td>-0.002235</td>\n      <td>0.002893</td>\n      <td>0.003484</td>\n      <td>-0.000423</td>\n      <td>-0.001191</td>\n      <td>-9.726858e-07</td>\n      <td>0.000829</td>\n      <td>0.001101</td>\n      <td>0.000949</td>\n      <td>-0.000448</td>\n      <td>-0.001937</td>\n      <td>0.004193</td>\n      <td>-0.005596</td>\n      <td>0.005729</td>\n      <td>0.002590</td>\n      <td>-0.000148</td>\n      <td>0.000524</td>\n      <td>0.000913</td>\n      <td>0.002615</td>\n      <td>0.000926</td>\n      <td>0.002903</td>\n      <td>0.003628</td>\n      <td>-0.006569</td>\n      <td>0.004073</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.003678</td>\n      <td>0.000033</td>\n      <td>-0.000118</td>\n      <td>0.000601</td>\n      <td>0.002501</td>\n      <td>-0.000906</td>\n      <td>-0.003302</td>\n      <td>0.002767</td>\n      <td>0.000897</td>\n      <td>0.002696</td>\n      <td>-0.000294</td>\n      <td>-0.000187</td>\n      <td>0.000332</td>\n      <td>0.000343</td>\n      <td>0.001432</td>\n      <td>0.008501</td>\n      <td>0.001922</td>\n      <td>0.000701</td>\n      <td>0.009646</td>\n      <td>-0.008233</td>\n      <td>0.002137</td>\n      <td>-0.000261</td>\n      <td>0.000952</td>\n      <td>0.000422</td>\n      <td>0.002201</td>\n      <td>0.002046</td>\n      <td>-0.000497</td>\n      <td>0.004838</td>\n      <td>-0.002786</td>\n      <td>0.002878</td>\n      <td>-0.000768</td>\n      <td>-0.000100</td>\n      <td>0.000304</td>\n      <td>0.000531</td>\n      <td>0.001196</td>\n      <td>0.001196</td>\n      <td>-0.000755</td>\n      <td>0.007316</td>\n      <td>0.005992</td>\n      <td>-0.001077</td>\n      <td>0.003149</td>\n      <td>-0.000217</td>\n      <td>0.000565</td>\n      <td>0.000831</td>\n      <td>-0.000833</td>\n      <td>-0.002148</td>\n      <td>-0.002115</td>\n      <td>0.004859</td>\n      <td>0.003605</td>\n      <td>0.001689</td>\n      <td>-0.000732</td>\n      <td>4.629588e-05</td>\n      <td>0.000902</td>\n      <td>0.001128</td>\n      <td>0.001179</td>\n      <td>-0.000376</td>\n      <td>-0.001399</td>\n      <td>0.006992</td>\n      <td>-0.003909</td>\n      <td>0.006095</td>\n      <td>0.002725</td>\n      <td>-0.000103</td>\n      <td>0.000571</td>\n      <td>0.001010</td>\n      <td>0.002984</td>\n      <td>0.001086</td>\n      <td>0.003995</td>\n      <td>0.004958</td>\n      <td>-0.006412</td>\n      <td>0.004215</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.004371</td>\n      <td>0.000176</td>\n      <td>-0.000093</td>\n      <td>0.000601</td>\n      <td>0.004180</td>\n      <td>0.002112</td>\n      <td>-0.002280</td>\n      <td>0.007179</td>\n      <td>0.001032</td>\n      <td>0.008132</td>\n      <td>-0.000018</td>\n      <td>-0.000162</td>\n      <td>0.000367</td>\n      <td>0.000436</td>\n      <td>0.005456</td>\n      <td>0.008548</td>\n      <td>0.001960</td>\n      <td>0.003087</td>\n      <td>0.010331</td>\n      <td>-0.006104</td>\n      <td>0.004205</td>\n      <td>-0.000252</td>\n      <td>0.001001</td>\n      <td>0.000599</td>\n      <td>0.006649</td>\n      <td>0.003969</td>\n      <td>-0.000344</td>\n      <td>0.006149</td>\n      <td>0.000641</td>\n      <td>0.006763</td>\n      <td>-0.000627</td>\n      <td>0.000001</td>\n      <td>0.000459</td>\n      <td>0.000578</td>\n      <td>0.005968</td>\n      <td>0.001727</td>\n      <td>-0.000120</td>\n      <td>0.013703</td>\n      <td>0.007163</td>\n      <td>0.002111</td>\n      <td>0.004469</td>\n      <td>-0.000148</td>\n      <td>0.000689</td>\n      <td>0.000944</td>\n      <td>0.000325</td>\n      <td>-0.001502</td>\n      <td>-0.001692</td>\n      <td>0.005098</td>\n      <td>0.003670</td>\n      <td>0.006881</td>\n      <td>0.000258</td>\n      <td>2.941638e-04</td>\n      <td>0.001270</td>\n      <td>0.001414</td>\n      <td>0.001259</td>\n      <td>-0.000208</td>\n      <td>-0.000760</td>\n      <td>0.010372</td>\n      <td>-0.001975</td>\n      <td>0.012714</td>\n      <td>0.004354</td>\n      <td>-0.000032</td>\n      <td>0.000656</td>\n      <td>0.001082</td>\n      <td>0.008255</td>\n      <td>0.002068</td>\n      <td>0.005477</td>\n      <td>0.006650</td>\n      <td>-0.006163</td>\n      <td>0.005146</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Examine coefficients of each estimator\n",
    "est_coefs = []\n",
    "for i, fc in enumerate(feat_cols):\n",
    "    fc_coefs = pd.Series([est.coef_[i] for est in cv_scores['estimator']], name=fc)\n",
    "    est_coefs.append(fc_coefs)\n",
    "coefs_df = pd.concat(est_coefs, axis=1)\n",
    "\n",
    "coefs_df.describe()"
   ]
  },
  {
   "source": [
    "## Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average Fit_time: 2.041666\n\nAverage Score_time: 0.001622\n\nAverage Test_score: -0.121144\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "cv_scores = run_cv(log_reg, X, y, scoring='neg_root_mean_squared_error', n_splits=5, verbose=0, return_estimators=True)\n",
    "display_results(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Attempt grid search to tune logistic regression model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "def grid_search_cv(est, param_grid, scoring, n_splits=5):\n",
    "    return GridSearchCV(est, param_grid=param_grid, scoring=scoring, cv=StratifiedKFold(n_splits=n_splits))\n",
    "\n",
    "def print_grid_search_scores(gs_cv_obj, param_grid):\n",
    "    # Find non-singleton elements of param_grid\n",
    "    ns_grid_params = [k for k in param_grid if len(param_grid[k]) > 1]\n",
    "\n",
    "    # Print mean test score for each parameter combination\n",
    "    if len(ns_grid_params) == 1:\n",
    "        index = [f\"{ns_grid_params[0]}_{p}\" for p in param_grid[ns_grid_params[0]]]\n",
    "        name = 'mean_test_root_mean_squared_error'\n",
    "        print(pd.DataFrame(gs_cv_obj.cv_results_['mean_test_score'], index=index, columns=[name]))\n",
    "    elif len(ns_grid_params) == 2:\n",
    "        idx_p = ns_grid_params[0]\n",
    "        col_p = ns_grid_params[1]\n",
    "        index = [f\"{idx_p}_{p}\" for p in param_grid[idx_p]]\n",
    "        columns = [f\"{col_p}_{p}\" for p in param_grid[col_p]]\n",
    "        print(pd.DataFrame(gs_cv_obj.cv_results_['mean_test_score'].reshape(len(index), len(columns)), index=index, columns=columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(),\n",
       "             param_grid={'max_iter': [1000], 'penalty': ['none'],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'sag', 'saga']},\n",
       "             scoring='neg_root_mean_squared_error')"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# No penalty\n",
    "log_reg = LogisticRegression()\n",
    "param_grid = {'penalty': ['none'],\n",
    "              'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "              'max_iter': [1000]}\n",
    "\n",
    "gs_cv_no_penalty = grid_search_cv(log_reg, param_grid, scoring='neg_root_mean_squared_error', n_splits=5)\n",
    "gs_cv_no_penalty.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                  mean_test_root_mean_squared_error\nsolver_newton-cg                          -0.121228\nsolver_lbfgs                              -0.121228\nsolver_sag                                -0.121144\nsolver_saga                               -0.121144\n"
     ]
    }
   ],
   "source": [
    "# Print results of training logistic regression w/ no penalty\n",
    "param_grid = {'penalty': ['none'],\n",
    "              'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "              'max_iter': [1000]}\n",
    "print_grid_search_scores(gs_cv_no_penalty, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pickle object\n",
    "with open('./test/log_reg_gs_no_penalty.pkl', 'wb') as f:\n",
    "    pickle.dump(gs_cv_no_penalty, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(),\n",
       "             param_grid={'C': [0.01, 0.02, 0.04, 0.1, 0.2, 0.4, 1.0, 2.0, 4.0,\n",
       "                               10.0],\n",
       "                         'max_iter': [1000], 'penalty': ['l2'],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag',\n",
       "                                    'saga']},\n",
       "             scoring='neg_root_mean_squared_error')"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "# Grid search w/ L2 penalty\n",
    "param_grid = {'penalty' : ['l2'],\n",
    "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "              'C': [0.01, 0.02, 0.04, 0.1, 0.2, 0.4, 1.0, 2.0, 4.0, 10.],\n",
    "              'max_iter': [1000],\n",
    "              }\n",
    "gs_cv_l2 = grid_search_cv(LogisticRegression(), param_grid, scoring='neg_root_mean_squared_error', n_splits=5)\n",
    "gs_cv_l2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pickle object\n",
    "with open('./test/log_reg_gs_l2.pkl', 'wb') as f:\n",
    "    pickle.dump(gs_cv_l2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                    C_0.01    C_0.02    C_0.04     C_0.1     C_0.2     C_0.4     C_1.0     C_2.0     C_4.0    C_10.0\nsolver_newton-cg -0.119053 -0.119053 -0.118405 -0.119053 -0.119053 -0.120215 -0.120302 -0.120302 -0.120215 -0.120215\nsolver_lbfgs     -0.121075 -0.121075 -0.121244 -0.121075 -0.121075 -0.121328 -0.121328 -0.121661 -0.121328 -0.121328\nsolver_liblinear -0.121228 -0.121228 -0.121578 -0.121228 -0.121228 -0.121228 -0.121228 -0.121479 -0.121228 -0.121228\nsolver_sag       -0.121228 -0.121144 -0.121312 -0.121228 -0.121228 -0.121144 -0.121144 -0.121228 -0.121144 -0.121228\nsolver_saga      -0.121144 -0.121228 -0.121228 -0.121144 -0.121228 -0.121228 -0.121228 -0.121228 -0.121144 -0.121144\n"
     ]
    }
   ],
   "source": [
    "# Print results of training logistic regression w/ L2 penalty\n",
    "param_grid = {'penalty' : ['l2'],\n",
    "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "              'C': [0.01, 0.02, 0.04, 0.1, 0.2, 0.4, 1.0, 2.0, 4.0, 10.],\n",
    "              'max_iter': [1000],\n",
    "              }\n",
    "print_grid_search_scores(gs_cv_l2, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/davidchung/opt/anaconda3/envs/runner-preds/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(),\n",
       "             param_grid={'C': [0.01, 0.02, 0.04, 0.1, 0.2, 0.4, 1.0, 2.0, 4.0,\n",
       "                               10.0],\n",
       "                         'max_iter': [1000], 'penalty': ['l1'],\n",
       "                         'solver': ['liblinear', 'saga']},\n",
       "             scoring='neg_root_mean_squared_error')"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Grid search w/ L1 penalty\n",
    "param_grid = {'penalty' : ['l1'],\n",
    "              'solver': ['liblinear', 'saga'],\n",
    "              'C': [0.01, 0.02, 0.04, 0.1, 0.2, 0.4, 1.0, 2.0, 4.0, 10.],\n",
    "              'max_iter': [1000],\n",
    "              }\n",
    "gs_cv_l1 = grid_search_cv(LogisticRegression(), param_grid, scoring='neg_root_mean_squared_error', n_splits=5)\n",
    "gs_cv_l1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                    C_0.01    C_0.02    C_0.04     C_0.1     C_0.2     C_0.4     C_1.0     C_2.0     C_4.0    C_10.0\nsolver_liblinear -0.116757 -0.116757 -0.116757 -0.116757 -0.116857 -0.116857 -0.119316 -0.119316 -0.120376 -0.120376\nsolver_saga      -0.121060 -0.121060 -0.121144 -0.121144 -0.121144 -0.121144 -0.121144 -0.121144 -0.121144 -0.121144\n"
     ]
    }
   ],
   "source": [
    "# Print results of training logistic regression w/ L1 penalty\n",
    "param_grid = {'penalty' : ['l1'],\n",
    "              'solver': ['liblinear', 'saga'],\n",
    "              'C': [0.01, 0.02, 0.04, 0.1, 0.2, 0.4, 1.0, 2.0, 4.0, 10.],\n",
    "              'max_iter': [1000],\n",
    "              }\n",
    "print_grid_search_scores(gs_cv_l1, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pickle object\n",
    "with open('./test/log_reg_gs_l1.pkl', 'wb') as f:\n",
    "    pickle.dump(gs_cv_l1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: feature selection (low variance, correlated features, univariate feature selection, pca, ...) (https://towardsdatascience.com/feature-selection-and-dimensionality-reduction-f488d1a035de)"
   ]
  },
  {
   "source": [
    "## Removing low variance features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((42766, 70), (42766, 70))"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "var_selector = VarianceThreshold()\n",
    "X_removed = var_selector.fit_transform(X)\n",
    "X_removed.shape, X.shape"
   ]
  },
  {
   "source": [
    "## Removing correlated features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['perceived trainingSuccess', 'perceived recovery', 'perceived trainingSuccess.1', 'perceived recovery.1', 'perceived trainingSuccess.2', 'perceived recovery.2', 'perceived trainingSuccess.3', 'perceived recovery.3', 'perceived trainingSuccess.4', 'perceived recovery.4', 'perceived trainingSuccess.5', 'perceived recovery.5', 'perceived trainingSuccess.6', 'perceived recovery.6'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "corr_thresh = 0.6\n",
    "corr_df = daily_df[feat_cols].corr()\n",
    "corr_values = np.triu(corr_df.values, k=1)\n",
    "high_corr_col_mask = np.any(np.abs(corr_values) > corr_thresh, axis=0)\n",
    "\n",
    "high_corr_cols = corr_df.columns[high_corr_col_mask]\n",
    "high_corr_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "len(high_corr_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "# TODO: re-train linear and logistic regression"
   ]
  }
 ]
}