{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd05637294743aae0e025520e0e9d1da7627837a77c00833e7443ec1a12437a0109",
   "display_name": "Python 3.9.5 64-bit ('runner-preds': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   nr. sessions  total km  km Z3-4  km Z5-T1-T2  km sprinting  strength training  hours alternative  perceived exertion  perceived trainingSuccess  perceived recovery  nr. sessions.1  total km.1  km Z3-4.1  km Z5-T1-T2.1  km sprinting.1  strength training.1  hours alternative.1  perceived exertion.1  perceived trainingSuccess.1  perceived recovery.1  nr. sessions.2  total km.2  km Z3-4.2  km Z5-T1-T2.2  km sprinting.2  strength training.2  hours alternative.2  perceived exertion.2  perceived trainingSuccess.2  perceived recovery.2  nr. sessions.3  total km.3  km Z3-4.3  km Z5-T1-T2.3  km sprinting.3  strength training.3  hours alternative.3  perceived exertion.3  perceived trainingSuccess.3  perceived recovery.3  nr. sessions.4  total km.4  km Z3-4.4  km Z5-T1-T2.4  km sprinting.4  strength training.4  hours alternative.4  perceived exertion.4  perceived trainingSuccess.4  perceived recovery.4  nr. sessions.5  total km.5  km Z3-4.5  km Z5-T1-T2.5  km sprinting.5  strength training.5  \\\n",
       "0           1.0       5.8      0.0          0.6           1.2                0.0               0.00                0.11                       0.00                0.18             0.0         0.0        0.0            0.0             0.0                  0.0                 0.00                 -0.01                        -0.01                 -0.01             1.0         0.0        0.0            0.0             0.0                  1.0                 0.00                  0.10                         0.00                  0.17             0.0         0.0        0.0            0.0             0.0                  0.0                 0.00                 -0.01                        -0.01                 -0.01             1.0         0.0        0.0            0.0             0.0                  0.0                 1.08                  0.08                         0.00                  0.18             1.0        16.4       10.0            0.0             0.0                  1.0   \n",
       "1           0.0       0.0      0.0          0.0           0.0                0.0               0.00               -0.01                      -0.01               -0.01             1.0         0.0        0.0            0.0             0.0                  1.0                 0.00                  0.10                         0.00                  0.17             0.0         0.0        0.0            0.0             0.0                  0.0                 0.00                 -0.01                        -0.01                 -0.01             1.0         0.0        0.0            0.0             0.0                  0.0                 1.08                  0.08                         0.00                  0.18             1.0        16.4       10.0            0.0             0.0                  1.0                 0.00                  0.11                         0.00                  0.17             1.0         0.0        0.0            0.0             0.0                  0.0   \n",
       "2           1.0       0.0      0.0          0.0           0.0                1.0               0.00                0.10                       0.00                0.17             0.0         0.0        0.0            0.0             0.0                  0.0                 0.00                 -0.01                        -0.01                 -0.01             1.0         0.0        0.0            0.0             0.0                  0.0                 1.08                  0.08                         0.00                  0.18             1.0        16.4       10.0            0.0             0.0                  1.0                 0.00                  0.11                         0.00                  0.17             1.0         0.0        0.0            0.0             0.0                  0.0                 1.00                  0.10                         0.00                  0.15             1.0         5.2        0.0            0.5             1.2                  0.0   \n",
       "3           0.0       0.0      0.0          0.0           0.0                0.0               0.00               -0.01                      -0.01               -0.01             1.0         0.0        0.0            0.0             0.0                  0.0                 1.08                  0.08                         0.00                  0.18             1.0        16.4       10.0            0.0             0.0                  1.0                 0.00                  0.11                         0.00                  0.17             1.0         0.0        0.0            0.0             0.0                  0.0                 1.00                  0.10                         0.00                  0.15             1.0         5.2        0.0            0.5             1.2                  0.0                 0.00                  0.10                         0.00                  0.17             0.0         0.0        0.0            0.0             0.0                  0.0   \n",
       "4           1.0       0.0      0.0          0.0           0.0                0.0               1.08                0.08                       0.00                0.18             1.0        16.4       10.0            0.0             0.0                  1.0                 0.00                  0.11                         0.00                  0.17             1.0         0.0        0.0            0.0             0.0                  0.0                 1.00                  0.10                         0.00                  0.15             1.0         5.2        0.0            0.5             1.2                  0.0                 0.00                  0.10                         0.00                  0.17             0.0         0.0        0.0            0.0             0.0                  0.0                 0.00                 -0.01                        -0.01                 -0.01             1.0         0.0        0.0            0.0             0.0                  1.0   \n",
       "\n",
       "   hours alternative.5  perceived exertion.5  perceived trainingSuccess.5  perceived recovery.5  nr. sessions.6  total km.6  km Z3-4.6  km Z5-T1-T2.6  km sprinting.6  strength training.6  hours alternative.6  perceived exertion.6  perceived trainingSuccess.6  perceived recovery.6  Athlete ID  injury  Date  \n",
       "0                  0.0                  0.11                         0.00                  0.17             1.0         0.0        0.0            0.0             0.0                  0.0                  1.0                  0.10                         0.00                  0.15           0       0     0  \n",
       "1                  1.0                  0.10                         0.00                  0.15             1.0         5.2        0.0            0.5             1.2                  0.0                  0.0                  0.10                         0.00                  0.17           0       0     1  \n",
       "2                  0.0                  0.10                         0.00                  0.17             0.0         0.0        0.0            0.0             0.0                  0.0                  0.0                 -0.01                        -0.01                 -0.01           0       0     2  \n",
       "3                  0.0                 -0.01                        -0.01                 -0.01             1.0         0.0        0.0            0.0             0.0                  1.0                  0.0                  0.10                         0.00                  0.17           0       0     3  \n",
       "4                  0.0                  0.10                         0.00                  0.17             1.0        17.6        7.2            0.0             0.0                  0.0                  0.0                  0.11                         0.00                  0.17           0       0     4  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nr. sessions</th>\n      <th>total km</th>\n      <th>km Z3-4</th>\n      <th>km Z5-T1-T2</th>\n      <th>km sprinting</th>\n      <th>strength training</th>\n      <th>hours alternative</th>\n      <th>perceived exertion</th>\n      <th>perceived trainingSuccess</th>\n      <th>perceived recovery</th>\n      <th>nr. sessions.1</th>\n      <th>total km.1</th>\n      <th>km Z3-4.1</th>\n      <th>km Z5-T1-T2.1</th>\n      <th>km sprinting.1</th>\n      <th>strength training.1</th>\n      <th>hours alternative.1</th>\n      <th>perceived exertion.1</th>\n      <th>perceived trainingSuccess.1</th>\n      <th>perceived recovery.1</th>\n      <th>nr. sessions.2</th>\n      <th>total km.2</th>\n      <th>km Z3-4.2</th>\n      <th>km Z5-T1-T2.2</th>\n      <th>km sprinting.2</th>\n      <th>strength training.2</th>\n      <th>hours alternative.2</th>\n      <th>perceived exertion.2</th>\n      <th>perceived trainingSuccess.2</th>\n      <th>perceived recovery.2</th>\n      <th>nr. sessions.3</th>\n      <th>total km.3</th>\n      <th>km Z3-4.3</th>\n      <th>km Z5-T1-T2.3</th>\n      <th>km sprinting.3</th>\n      <th>strength training.3</th>\n      <th>hours alternative.3</th>\n      <th>perceived exertion.3</th>\n      <th>perceived trainingSuccess.3</th>\n      <th>perceived recovery.3</th>\n      <th>nr. sessions.4</th>\n      <th>total km.4</th>\n      <th>km Z3-4.4</th>\n      <th>km Z5-T1-T2.4</th>\n      <th>km sprinting.4</th>\n      <th>strength training.4</th>\n      <th>hours alternative.4</th>\n      <th>perceived exertion.4</th>\n      <th>perceived trainingSuccess.4</th>\n      <th>perceived recovery.4</th>\n      <th>nr. sessions.5</th>\n      <th>total km.5</th>\n      <th>km Z3-4.5</th>\n      <th>km Z5-T1-T2.5</th>\n      <th>km sprinting.5</th>\n      <th>strength training.5</th>\n      <th>hours alternative.5</th>\n      <th>perceived exertion.5</th>\n      <th>perceived trainingSuccess.5</th>\n      <th>perceived recovery.5</th>\n      <th>nr. sessions.6</th>\n      <th>total km.6</th>\n      <th>km Z3-4.6</th>\n      <th>km Z5-T1-T2.6</th>\n      <th>km sprinting.6</th>\n      <th>strength training.6</th>\n      <th>hours alternative.6</th>\n      <th>perceived exertion.6</th>\n      <th>perceived trainingSuccess.6</th>\n      <th>perceived recovery.6</th>\n      <th>Athlete ID</th>\n      <th>injury</th>\n      <th>Date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>5.8</td>\n      <td>0.0</td>\n      <td>0.6</td>\n      <td>1.2</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>0.18</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.08</td>\n      <td>0.08</td>\n      <td>0.00</td>\n      <td>0.18</td>\n      <td>1.0</td>\n      <td>16.4</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.08</td>\n      <td>0.08</td>\n      <td>0.00</td>\n      <td>0.18</td>\n      <td>1.0</td>\n      <td>16.4</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.15</td>\n      <td>1.0</td>\n      <td>5.2</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>1.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.08</td>\n      <td>0.08</td>\n      <td>0.00</td>\n      <td>0.18</td>\n      <td>1.0</td>\n      <td>16.4</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.15</td>\n      <td>1.0</td>\n      <td>5.2</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>1.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.08</td>\n      <td>0.08</td>\n      <td>0.00</td>\n      <td>0.18</td>\n      <td>1.0</td>\n      <td>16.4</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.15</td>\n      <td>1.0</td>\n      <td>5.2</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>1.2</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.08</td>\n      <td>0.08</td>\n      <td>0.00</td>\n      <td>0.18</td>\n      <td>1.0</td>\n      <td>16.4</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.15</td>\n      <td>1.0</td>\n      <td>5.2</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>1.2</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>1.0</td>\n      <td>17.6</td>\n      <td>7.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.11</td>\n      <td>0.00</td>\n      <td>0.17</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Read in daily data\n",
    "data_path = os.path.join('.', 'data')\n",
    "daily_data_path = os.path.join(data_path, 'day_approach_maskedID_timeseries.csv')\n",
    "weekly_data_path = os.path.join(data_path, 'week_approach_maskedID_timeseries.csv')\n",
    "\n",
    "daily_df = pd.read_csv(daily_data_path)\n",
    "daily_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model trainer class\n",
    "from utils import Trainer\n",
    "\n",
    "test_split=0.2\n",
    "cv=5\n",
    "scoring='f1'\n",
    "\n",
    "ignore_cols = ['Athlete ID']\n",
    "model_trainer = Trainer(daily_df, 'injury', ignore_cols=ignore_cols, test_split=test_split)"
   ]
  },
  {
   "source": [
    "## Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model type:  log_reg\n",
      "Scaling:  standard\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('model', LogisticRegression())])\n",
      "Model type:  log_reg\n",
      "Scaling:  standard\n",
      "Average test recall score:  0.0\n"
     ]
    }
   ],
   "source": [
    "model_trainer.add_model('standard', 'log_reg')\n",
    "print(model_trainer.models[('log_reg', 'standard')])\n",
    "\n",
    "model_trainer.cross_validate('standard', 'log_reg', cv=cv, scoring=scoring, return_train_score=True, return_estimator=True)\n",
    "print('Average test recall score: ', model_trainer.scores[('log_reg', 'standard')]['test_score'].mean())"
   ]
  },
  {
   "source": [
    "## Complement Naive-Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model type:  comp_nb\n",
      "Scaling:  min_max\n",
      "Pipeline(steps=[('scaler', MinMaxScaler()), ('model', ComplementNB())])\n",
      "Model type:  comp_nb\n",
      "Scaling:  min_max\n",
      "Average test recall score:  0.03689406975124774\n"
     ]
    }
   ],
   "source": [
    "model_trainer.add_model('min_max', 'comp_nb')\n",
    "print(model_trainer.models[('comp_nb', 'min_max')])\n",
    "\n",
    "model_trainer.cross_validate('min_max', 'comp_nb', cv=cv, scoring=scoring, return_train_score=True, return_estimator=True)\n",
    "print('Average test recall score: ', model_trainer.scores[('comp_nb', 'min_max')]['test_score'].mean())"
   ]
  },
  {
   "source": [
    "## K-Nearest Neighbors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model type:  knn\n",
      "Scaling:  min_max\n",
      "Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('model',\n",
      "                 KNeighborsClassifier(n_neighbors=10, weights='distance'))])\n",
      "Model type:  knn\n",
      "Scaling:  min_max\n",
      "Average test recall score:  0.0\n"
     ]
    }
   ],
   "source": [
    "weights='distance'\n",
    "n_neighbors=10\n",
    "model_trainer.add_model('min_max', 'knn', n_neighbors=n_neighbors, weights=weights)\n",
    "print(model_trainer.models[('knn', 'min_max')])\n",
    "\n",
    "model_trainer.cross_validate('min_max', 'knn', cv=cv, scoring=scoring, return_train_score=True, return_estimator=True)\n",
    "print('Average test recall score: ', model_trainer.scores[('knn', 'min_max')]['test_score'].mean())"
   ]
  },
  {
   "source": [
    "## Support Vector Machine"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model type:  svm\n",
      "Scaling:  min_max\n",
      "Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('model', SVC(C=5.0, class_weight='balanced'))])\n",
      "Model type:  svm\n",
      "Scaling:  min_max\n",
      "Average test recall score:  0.03774138389402519\n"
     ]
    }
   ],
   "source": [
    "C=5.0\n",
    "class_weight='balanced'\n",
    "kernel='rbf'\n",
    "model_trainer.add_model('min_max', 'svm', C=C, class_weight=class_weight, kernel=kernel)\n",
    "print(model_trainer.models[('svm', 'min_max')])\n",
    "\n",
    "model_trainer.cross_validate('min_max', 'svm', cv=cv, scoring=scoring, return_train_score=True, return_estimator=True)\n",
    "print('Average test recall score: ', model_trainer.scores[('svm', 'min_max')]['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.05076142, 0.01891253, 0.02659574, 0.04494382, 0.0474934 ])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "model_trainer.scores[('svm', 'min_max')]['test_score']"
   ]
  },
  {
   "source": [
    "## Random Forest Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model type:  rf\n",
      "Scaling:  None\n",
      "Pipeline(steps=[('model',\n",
      "                 RandomForestClassifier(bootstrap=False,\n",
      "                                        class_weight='balanced',\n",
      "                                        criterion='entropy'))])\n",
      "Model type:  rf\n",
      "Scaling:  None\n",
      "Average test recall score:  0.0\n"
     ]
    }
   ],
   "source": [
    "class_weight='balanced'\n",
    "criterion='entropy'\n",
    "bootstrap=False\n",
    "model_trainer.add_model(None, 'rf', class_weight=class_weight, bootstrap=bootstrap, criterion=criterion)\n",
    "print(model_trainer.models[('rf', None)])\n",
    "\n",
    "model_trainer.cross_validate(None, 'rf', cv=cv, scoring=scoring, return_train_score=True, return_estimator=True)\n",
    "print('Average test recall score: ', model_trainer.scores[('rf', None)]['test_score'].mean())"
   ]
  },
  {
   "source": [
    "## XGBoost Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model type:  xgb\n",
      "Scaling:  None\n",
      "Pipeline(steps=[('model',\n",
      "                 XGBClassifier(base_score=None, booster=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, gamma=None, gpu_id=None,\n",
      "                               importance_type='gain',\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_delta_step=None, max_depth=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=100,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               random_state=None, reg_alpha=None,\n",
      "                               reg_lambda=None, scale_pos_weight=None,\n",
      "                               subsample=None, tree_method=None,\n",
      "                               use_label_encoder=False,\n",
      "                               validate_parameters=None, verbosity=None))])\n",
      "Model type:  xgb\n",
      "Scaling:  None\n",
      "[23:08:53] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:09:00] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:09:06] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:09:12] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:09:19] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Average test recall score:  0.01263157894736842\n"
     ]
    }
   ],
   "source": [
    "use_label_encoder=False\n",
    "model_trainer.add_model(None, 'xgb', use_label_encoder=False)\n",
    "print(model_trainer.models[('xgb', None)])\n",
    "\n",
    "def f1_xgb(y_pred, d_train):\n",
    "    y_true = d_train.get_label()\n",
    "    return 'f1', f1_score(y_true, y_pred)\n",
    "\n",
    "fit_params = {'eval_metric' : f1_xgb}\n",
    "model_trainer.cross_validate(None, 'xgb', fit_params=fit_params, cv=cv, scoring=scoring, return_train_score=True, return_estimator=True)\n",
    "print('Average test recall score: ', model_trainer.scores[('xgb', None)]['test_score'].mean())"
   ]
  },
  {
   "source": [
    "# Imbalanced data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Undersampling methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### a. Cluster centroids"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model trainer class for imbalanced data\n",
    "from utils import ImbTrainer\n",
    "\n",
    "test_split=0.2\n",
    "cv=5\n",
    "scoring='f1'\n",
    "\n",
    "ignore_cols = ['Athlete ID']\n",
    "model_trainer = ImbTrainer(daily_df, 'injury', ignore_cols=ignore_cols, test_split=test_split)"
   ]
  },
  {
   "source": [
    "### Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model type:  log_reg\n",
      "Scaling:  standard\n",
      "Sampler type:  clust_cents\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('sampler', ClusterCentroids()),\n",
      "                ('model', LogisticRegression())])\n",
      "Model type:  log_reg\n",
      "Scaling:  standard\n",
      "Sampler type:  clust_cents\n",
      "Average test recall score:  0.03595304566786586\n"
     ]
    }
   ],
   "source": [
    "model_trainer.add_model('standard', 'clust_cents', 'log_reg')\n",
    "print(model_trainer.models[('log_reg', 'standard', 'clust_cents')])\n",
    "\n",
    "model_trainer.cross_validate('standard', 'clust_cents', 'log_reg', cv=cv, scoring=scoring, return_train_score=True, return_estimator=True)\n",
    "print(f'Average test {scoring.upper()} score: ', model_trainer.scores[('log_reg', 'standard', 'clust_cents')]['test_score'].mean())"
   ]
  },
  {
   "source": [
    "### K-Nearest Neighbors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model type:  knn\n",
      "Scaling:  min_max\n",
      "Sampler type:  clust_cents\n",
      "Pipeline(steps=[('scaler', MinMaxScaler()), ('sampler', ClusterCentroids()),\n",
      "                ('model', KNeighborsClassifier(weights='distance'))])\n",
      "Model type:  knn\n",
      "Scaling:  min_max\n",
      "Sampler type:  clust_cents\n",
      "Average test F1 score:  0.024458803701581423\n"
     ]
    }
   ],
   "source": [
    "model_params = {'weights': 'distance',\n",
    "                'n_neighbors': 5}\n",
    "model_trainer.add_model('min_max', 'clust_cents', 'knn', model_params=model_params)\n",
    "print(model_trainer.models[('knn', 'min_max', 'clust_cents')])\n",
    "\n",
    "model_trainer.cross_validate('min_max', 'clust_cents', 'knn', cv=cv, scoring=scoring, return_train_score=True, return_estimator=True)\n",
    "print(f'Average test {scoring.upper()} score: ', model_trainer.scores[('knn', 'min_max', 'clust_cents')]['test_score'].mean())"
   ]
  },
  {
   "source": [
    "### Support Vector Machine"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model type:  svm\n",
      "Scaling:  min_max\n",
      "Sampler type:  clust_cents\n",
      "Pipeline(steps=[('scaler', MinMaxScaler()), ('sampler', ClusterCentroids()),\n",
      "                ('model', SVC())])\n",
      "Model type:  svm\n",
      "Scaling:  min_max\n",
      "Sampler type:  clust_cents\n",
      "Average test recall score:  0.036295151443745635\n"
     ]
    }
   ],
   "source": [
    "# C=5.0\n",
    "# class_weight='balanced'\n",
    "# kernel='rbf'\n",
    "model_trainer.add_model('min_max', 'clust_cents', 'svm')\n",
    "print(model_trainer.models[('svm', 'min_max', 'clust_cents')])\n",
    "\n",
    "model_trainer.cross_validate('min_max', 'clust_cents', 'svm', cv=cv, scoring=scoring, return_train_score=True, return_estimator=True)\n",
    "print(f'Average test {scoring.upper()} score: ', model_trainer.scores[('svm', 'min_max', 'clust_cents')]['test_score'].mean())"
   ]
  },
  {
   "source": [
    "### Random Forests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model type:  rf\n",
      "Scaling:  min_max\n",
      "Sampler type:  clust_cents\n",
      "Pipeline(steps=[('scaler', MinMaxScaler()), ('sampler', ClusterCentroids()),\n",
      "                ('model', RandomForestClassifier(bootstrap=False))])\n",
      "Model type:  rf\n",
      "Scaling:  min_max\n",
      "Sampler type:  clust_cents\n",
      "Average test recall score:  0.02693425384906928\n"
     ]
    }
   ],
   "source": [
    "# class_weight='balanced'\n",
    "# criterion='entropy'\n",
    "# bootstrap=False\n",
    "model_params = {'bootstrap': False}\n",
    "model_trainer.add_model('min_max', 'clust_cents', 'rf', model_params)\n",
    "print(model_trainer.models[('rf', 'min_max', 'clust_cents')])\n",
    "\n",
    "model_trainer.cross_validate('min_max', 'clust_cents', 'rf', cv=cv, scoring=scoring, return_train_score=True, return_estimator=True)\n",
    "print(f'Average test {scoring.upper()} score: ', model_trainer.scores[('rf', 'min_max', 'clust_cents')]['test_score'].mean())"
   ]
  },
  {
   "source": [
    "## Test out cluster centroids"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "cc = ClusterCentroids()\n",
    "test_X_res, test_y_res = cc.fit_resample(model_trainer.X_train, model_trainer.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of negative samples:  33747\nNumber of positive samples:  467\n"
     ]
    }
   ],
   "source": [
    "print('Number of negative samples: ', (model_trainer.y_train == 0).sum())\n",
    "print('Number of positive samples: ', (model_trainer.y_train == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of negative samples:  467\nNumber of positive samples:  467\n"
     ]
    }
   ],
   "source": [
    "print('Number of negative samples: ', (test_y_res == 0).sum())\n",
    "print('Number of positive samples: ', (test_y_res == 1).sum())"
   ]
  },
  {
   "source": [
    "## Feature selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: low variance features, correlated features, univariate feature selection, pca -> https://towardsdatascience.com/feature-selection-and-dimensionality-reduction-f488d1a035de"
   ]
  }
 ]
}